import express from 'express';
import cors from 'cors';
import OpenAI from 'openai';
import dotenv from 'dotenv';
import { systemPrompt as emperorPrompt } from './prompts/emperor.js';

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json());

// ç¡…åŸºæµåŠ¨ APIï¼ˆOpenAI å…¼å®¹æŽ¥å£ï¼‰
const client = new OpenAI({
  apiKey: process.env.SILICONFLOW_API_KEY,
  baseURL: process.env.SILICONFLOW_BASE_URL || 'https://api.siliconflow.cn/v1'
});

// æ¨¡å¼å¯¹åº”çš„ system prompt
const modePrompts = {
  emperor: emperorPrompt
};

// èŠå¤©æŽ¥å£
app.post('/api/chat', async (req, res) => {
  try {
    const { message, mode = 'emperor', history = [] } = req.body;
    
    const systemPrompt = modePrompts[mode] || modePrompts.emperor;
    
    const messages = [
      { role: 'system', content: systemPrompt },
      ...history,
      { role: 'user', content: message }
    ];

    const completion = await client.chat.completions.create({
      model: 'Qwen/Qwen2.5-7B-Instruct', // ç¡…åŸºæµåŠ¨å…è´¹æ¨¡åž‹
      messages,
      temperature: 0.8,
      max_tokens: 2000
    });

    const reply = completion.choices[0].message.content;
    
    res.json({
      success: true,
      reply,
      mode
    });
  } catch (error) {
    console.error('Chat error:', error);
    res.status(500).json({
      success: false,
      error: error.message
    });
  }
});

// èŽ·å–å¯ç”¨æ¨¡å¼
app.get('/api/modes', (req, res) => {
  res.json({
    modes: [
      {
        id: 'emperor',
        name: 'ä¸‰å›½è°‹å£«å›¢',
        icon: 'ðŸ‘‘',
        description: 'åŒ–èº«ä¸»å…¬ï¼Œè®©ä¸‰å›½åè‡£ä¸ºä½ å‡ºè°‹åˆ’ç­–',
        characters: ['æ€»ç®¡Â·é˜¿å®', 'å†›å¸ˆÂ·è¯¸è‘›äº®', 'å…µæ³•å®¶Â·å­™æ­¦', 'æ–­æ¡ˆå®˜Â·ç‹„ä»æ°']
      }
    ]
  });
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`ðŸš€ Server running on http://localhost:${PORT}`);
});
